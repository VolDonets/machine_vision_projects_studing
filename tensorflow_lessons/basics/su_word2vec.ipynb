{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's define some constants first:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "vocabulary_size = 50000\n",
    "embedding_size = 128    # dimension of the embedding spase\n",
    "num_sampled = 64        # number of negative examples to sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-4a7ec789b583>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreverse_dictionary\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Number of training examples:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Number of validation examples:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data, val_data, reverse_dictionary = load_data()\n",
    "print(\"Number of training examples:\", len(train_data) * batch_size)\n",
    "print(\"Number of validation examples:\", len(val_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def skipgram():\n",
    "    batch_inputs = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
    "    batch_labes = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    val_dataset = tf.constant(val_data, dtype=tf.int32)\n",
    "\n",
    "    with tf.variable_creator_scope('word2vec') as scope:\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        batch_embeddings = tf.nn.embedding_lookup(embeddings, batch_inputs)\n",
    "\n",
    "        weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev = 1.0 / math.sqrt(embedding_size)))\n",
    "        biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=weights,\n",
    "                                             biases=biases,\n",
    "                                             labels=batch_labes,\n",
    "                                             inputs=batch_inputs,\n",
    "                                             num_sampled=num_sampled,\n",
    "                                             num_classes=vocabulary_size))\n",
    "\n",
    "        norm = tf.sqrt(tf.reduce_mean(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "\n",
    "        val_embeddings = tf.nn.embedding_lookup(normalized_embeddings, val_dataset)\n",
    "        similarity = tf.matmul(val_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        return batch_inputs, batch_labes, normalized_embeddings, loss, similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def run():\n",
    "    batch_inputs, batch_labels, normalized_embeddings, loss, similarity = skipgram()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        average_loss = 0.0\n",
    "        for step, batch_data in enumerate(train_data):\n",
    "            inputs, labels = batch_data\n",
    "            feed_dict = {batch_inputs: inputs, batch_labels: labels}\n",
    "\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 1000\n",
    "                print_function('loss at iter', step, ':', average_loss)\n",
    "                average_loss = 0\n",
    "            if step % 5000 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in xrange(len(val_data)):\n",
    "                    top_k = 8\n",
    "                    nearest = (-sim([i, :]).argsort()[1:top_k+1])\n",
    "                    print_closest_words(val_data[i], nearest, reverse_dictionary)\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "finale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}